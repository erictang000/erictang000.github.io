---
layout: default
title: Home
---
<head>
  <link href="images/fontawesome/css/fontawesome.css" rel="stylesheet">
  <link href="images/fontawesome/css/brands.css" rel="stylesheet">
  <link href="images/fontawesome/css/solid.css" rel="stylesheet">
</head>
<body style="padding-top: 50px;">
  <div class="grid">
    <div class="col-1-3" style="width: 75%;">
      <div class="rowLeftFlexBox">      
        <h2 style="align-content: left;"><span>Eric Tang</span></h2>
      </div>
      <div class="rowLeftFlexBox">      
        <img src="images/me.jpeg"
          alt="Avatar"
          style="width: 75%;" />
      </div>
      <div class="rowLeftFlexBox">      
        <div style="font-size: 19px;">
          <a href="https://www.linkedin.com/in/erictang000"><i class="fab fa-linkedin"></i></a>
          <a href="https://www.github.com/erictang000"><i class="fab fa-github"></i></a>
          <a href="https://www.twitter.com/erictang000"><i class="fab fa-twitter"></i></a>
          <a href="images/EricTangResume2023.pdf"><i class="fas fa-file"></i></a>
        </div>
      </div>
    </div>
    <div class="col-1-3" style="width:180%; font-size: 15px;">
      <h2><span>About Me</span></h2>
      <p>I'm currently an MS student studying Computer Science at Stanford, advised by <a href="https://www.niebles.net" target="_blank" rel="noopener noreferrer">Juan Carlos Niebles</a> 
        at the Stanford Vision and Learning Lab. Previously I graduated from UC Berkeley with a BS in EECS, where I worked with <a href="https://people.eecs.berkeley.edu/~hendrycks/" target="_blank" rel="noopener noreferrer">Dan Hendrycks</a> and was advised by 
        <a href="http://people.eecs.berkeley.edu/~dawnsong/" target="_blank" rel="noopener noreferrer">Dawn Song</a> and 
        <a href="https://jsteinhardt.stat.berkeley.edu/" target="_blank" rel="noopener noreferrer">Jacob Steinhardt</a>. I've also interned at 
        <a href="https://www.meta.com/" target="_blank" rel="noopener noreferrer">Meta</a>, where I worked on scaling ads ranking models,
        and most recently at <a href="https://www.tiktok.com/about" target="_blank" rel="noopener noreferrer">TikTok</a>, where I worked on video understanding for creator tools and recommendation.
      </p>
      <p>
      I am broadly interested in computer vision, especially work in the area of efficient video understanding and representation learning.
      
      This site hosts my course and personal projects that I thought were cool, notes about what 
      I've been reading and learning about, and some of the teaching resources for I've compiled over the years!
      </p>
      <div style="font-size: 18px;">
        <a href="/notes/ml_notes">Notes</a> /
        <a href="/projects">Projects</a> /
        <a href="/teaching">Teaching</a>
      </div>
      <h2><span>Publications</span></h2>
      <div class="grid">
        <div class="col-1-2" style="width:180%; font-size: 15px;">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
              <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='../images/v2v.png' width="160">
                  </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                      <papertitle>How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios</papertitle>
                  <br>
                  Mantas Mazeika*,
                  <strong>Eric Tang*</strong>,
                  Andy Zou,
                  Steven Basart,
                  Jun Shern Chen,
                  Dawn Song,
                  David Forsyth,
                  Jacob Steinhardt,
                  Dan Hendrycks
                  <br>
                  <em>NeurIPS (Datasets and Benchmarks Track), 2022 <i>(Oral)</i></em>
                  <br>
                  <p>[<a href="https://openreview.net/forum?id=jbdp9m7nr0R">paper</a>][<a href="https://github.com/hendrycks/emodiversity">code</a>]</p>
                  <p></p>
                  <p>
                      Introduces the VCE and V2V datasets for understanding emotional response and viewer wellbeing for video data.
                  </p>
                  </td>
              </tr> 
              <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='../images/math.png' width="160">
                  </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                      <papertitle>Measuring Mathematical Problem Solving with the MATH Dataset</papertitle>
                  <br>
                  Dan Hendrycks,
                  Collin Burns,
                  Saurav Kadavath,
                  Akul Arora,
                  Steven Basart,
                  <strong>Eric Tang</strong>,
                  Dawn Song,
                  Jacob Steinhardt
                  <br>
                  <em>NeurIPS (Datasets and Benchmarks Track)</em>, 2021
                  <br>
                  <p>[<a href="https://arxiv.org/pdf/2103.03874.pdf">paper</a>][<a href="https://github.com/hendrycks/math">code</a>]</p>
                  <p></p>
                  <p>
                      Introduces the MATH dataset for measuring mathmatical reasoning in large language models, and the AMPS dataset for pretraining.
                  </p>
                  </td>
              </tr> 
              <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                      <img src='../images/tunable.gif' width="160">
                  </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                      <papertitle>Imaging Reconfigurable Molecular Concentration on a Graphene Field-Effect Transistor</papertitle>
                  <br>
                  Franklin Liou, Hsin-Zon Tsai, Andrew S Aikawa, Kyler C Natividad,
                  <strong>Eric Tang</strong>,
                  Ethan Ha, Alexander Riss, Kenji Watanabe, Takashi Taniguchi, Johannes Lischner, Alex Zettl, Michael F Crommie
                  <br>
                  <em>Nano Letters</em>, 2021
                  <br>
                  <p>[<a href="https://pubs.acs.org/doi/full/10.1021/acs.nanolett.1c03039">paper</a>]</p>
                  <p></p>
                  <p>
                      Uses scanning tunneling microscopy to demonstrate that molecules deposited onto graphene field-effect transistors (FETs) exhibit reversible, electrically tunable surface concentration.
                  </p>
                  </td>
              </tr> 
              </tbody>
          </table>
        </div>
      </div>
    </div>
  </div>
</body>