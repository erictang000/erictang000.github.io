---
layout: post
title: Reccommender Systems
--- 

## Work in Progress
<!-- ## Social Media Reccommender Systems and AI Alignment
#### July 2021
Remember how toxic Facebook newsfeeds got around the 2016 election? I remember an endless stream of videos from 'NowThis', and in general an overwhelming amount of political content and arguments.

and I think that this experimentation at such a politically volatile time was likely pretty harmful, at least to the general well being of users of the platform, if not to the state of US poltical discourse. This was before Facebook rolled out a number of news feed updates, including ones to encourage 'Meaningful Interactions', and likely scaled back pure watch time as a metric for the ranking of posts

Anecdotally, I've found the newsfeed to be somewhat better than before - although I use it much less than I used to. Most of my feed is from groups that I joined at some point, or from people that I actually know, and if I were the type to still post on Facebook (a boomer), I'd be at the very least slightly more likely to do so now than before. I trust that the 'Meaningful Interactions' update did probably result in an uptick of user comments and messages between users. Maybe there's a heavier filter for political content now as well, since it seems like those also did encourage people to yell at each other in the comments - although I suppose that might fall under the categorization of interaction that isn't super meaningful. 

Still, even if companies have somewhat updated their reccommender systems to be somewhat less malicious and aggresive in their attempts to strong-arm users out of their 

### How do we align reccommender systems with our values?
What are the values we want them to align with?
### Well being vs Pleasantness
What does it mean for something to be increase well being? What kind of videos would I believe to maximize my well being? Framing this within the context of social media reccommender systems is somewhat strange, although it does provide a compelling argument for understanding it being something that's overall beneficial to society.

For example, I might feel better now if I watch a video that is more pleasant than one that is less pleasant. But does that affect the long term well being that I want to be able to maximize? Probably not. If Facebook and Youtube maximized 'pleasantness', we would just get a bunch of cat and dog videos, which are nice, but don't contribute to well being in the long term -- in fact, it's possible I'd just spend more time on the platforms watching dog and cat videos, which could be bad for overall wellness long term.

If we want to do some kind of AI alignment for video valence understanding, I think a more compelling way to frame it is outside of the context of social media reccommender systems. The space is too fraught with questions like whether or not there are cultural of individual differences in what defines well being, or in whether spending time on the platforms is good at all.

I think a more compelling way of selling this is that we generally want AI systems to be able to understand how people will feel upon seeing some content, and that we can produce a dataset that will test the ability of models to do so while trying to eliminate spurious cues, by picking things that seem similar to a vision system but are easily picked up on by a human (we know a picture of JFK is nostalgic, but since it's not a cartoon, models have a harder time doing so). 

An example use case could be in robotic agents that have to interact in the real world sometime in the not-so distant future - if they see a scenario, they should try to have the ability to predcit how their human manager would feel about it without having some kind of hard coding done for what their human owner would feel -- they could then adjust their interactions accordingly without having to do facial recognition necessarily on the owner.

Well being includes both current well being, as well as long term well being - maximizing  -->


